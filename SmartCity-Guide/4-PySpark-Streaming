4)Τώρα ας δημιουργήσουμε το PySpark Streaming application για επεξεργασία δεδομένων:
---
"""
Smart City PySpark Structured Streaming Processor
Επεξεργασία ροών δεδομένων από Kafka σε πραγματικό χρόνο
"""

from pyspark.sql import SparkSession
from pyspark.sql.functions import (
    from_json, col, current_timestamp, window, avg, min, max, stddev,
    count, when, expr, to_timestamp, udf
)
from pyspark.sql.types import (
    StructType, StructField, StringType, DoubleType, 
    BooleanType, TimestampType, MapType
)
import logging

# Logging configuration
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class SmartCityStreamProcessor:
    """PySpark Streaming processor για Smart City δεδομένα"""
    
    def __init__(self, kafka_servers="localhost:9092", 
                 postgres_url="jdbc:postgresql://localhost:5432/smartcity",
                 postgres_user="smartcity_user",
                 postgres_password="SmartCity2024!"):
        
        self.kafka_servers = kafka_servers
        self.postgres_url = postgres_url
        self.postgres_user = postgres_user
        self.postgres_password = postgres_password
        self.spark = None
        
        # Threshold configurations για alerts
        self.thresholds = {
            'temperature': {'min': 0, 'max': 40},
            'air_quality': {'min': 0, 'max': 150},
            'pm25': {'min': 0, 'max': 35},
            'pm10': {'min': 0, 'max': 50},
            'noise': {'min': 30, 'max': 85},
            'co2': {'min': 300, 'max': 1000},
            'humidity': {'min': 20, 'max': 90}
        }
    
    def initialize_spark(self):
        """Αρχικοποίηση Spark Session"""
        logger.info("Αρχικοποίηση Spark Session...")
        
        self.spark = SparkSession.builder \
            .appName("SmartCityStreamProcessor") \
            .config("spark.sql.streaming.checkpointLocation", "/tmp/spark-checkpoint") \
            .config("spark.jars.packages", 
                    "org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0,"
                    "org.postgresql:postgresql:42.7.0") \
            .config("spark.sql.adaptive.enabled", "true") \
            .config("spark.sql.adaptive.coalescePartitions.enabled", "true") \
            .getOrCreate()
        
        self.spark.sparkContext.setLogLevel("WARN")
        logger.info("Spark Session αρχικοποιήθηκε επιτυχώς")
    
    def define_schema(self):
        """Ορισμός schema για Kafka messages"""
        return StructType([
            StructField("sensor_id", StringType(), False),
            StructField("sensor_type", StringType(), False),
            StructField("timestamp", StringType(), False),
            StructField("location", MapType(StringType(), StringType()), True),
            StructField("value", DoubleType(), False),
            StructField("unit", StringType(), True),
            StructField("is_anomaly", BooleanType(), True),
            StructField("status", StringType(), True)
        ])
    
    def read_kafka_stream(self, topic="smart-city-sensors"):
        """Ανάγνωση streaming data από Kafka"""
        logger.info(f"Σύνδεση σε Kafka topic: {topic}")
        
        df = self.spark \
            .readStream \
            .format("kafka") \
            .option("kafka.bootstrap.servers", self.kafka_servers) \
            .option("subscribe", topic) \
            .option("startingOffsets", "latest") \
            .option("failOnDataLoss", "false") \
            .load()
        
        # Parse JSON data
        schema = self.define_schema()
        
        parsed_df = df.select(
            from_json(col("value").cast("string"), schema).alias("data"),
            col("timestamp").alias("kafka_timestamp")
        ).select("data.*", "kafka_timestamp")
        
        # Convert timestamp string to timestamp type
        parsed_df = parsed_df.withColumn(
            "timestamp",
            to_timestamp(col("timestamp"))
        )
        
        # Extract location fields
        parsed_df = parsed_df \
            .withColumn("location_lat", col("location.lat").cast("double")) \
            .withColumn("location_lon", col("location.lon").cast("double")) \
            .withColumn("city", col("location.city"))
        
        logger.info("Kafka stream parsed επιτυχώς")
        return parsed_df
    
    def detect_anomalies(self, df):
        """Ανίχνευση ανωμαλιών με statistical methods"""
        
        # Υπολογισμός z-score για κάθε sensor type
        windowed_stats = df \
            .withWatermark("timestamp", "10 minutes") \
            .groupBy(
                window(col("timestamp"), "5 minutes"),
                col("sensor_type")
            ) \
            .agg(
                avg("value").alias("mean_value"),
                stddev("value").alias("stddev_value")
            )
        
        # Join με original data για z-score calculation
        df_with_stats = df.join(
            windowed_stats,
            (df.sensor_type == windowed_stats.sensor_type) &
            (df.timestamp >= windowed_stats.window.start) &
            (df.timestamp <= windowed_stats.window.end),
            "left"
        )
        
        # Calculate z-score και flag anomalies (|z| > 3)
        df_with_anomalies = df_with_stats.withColumn(
            "z_score",
            when(col("stddev_value") > 0,
                 (col("value") - col("mean_value")) / col("stddev_value")
            ).otherwise(0)
        ).withColumn(
            "is_statistical_anomaly",
            (col("z_score") > 3) | (col("z_score") < -3)
        )
        
        # Combine με original anomaly flag
        df_with_anomalies = df_with_anomalies.withColumn(
            "is_anomaly",
            col("is_anomaly") | col("is_statistical_anomaly")
        )
        
        return df_with_anomalies
    
    def check_thresholds(self, df):
        """Έλεγχος thresholds και δημιουργία alerts"""
        
        def check_threshold_udf(sensor_type, value):
            if sensor_type in self.thresholds:
                threshold = self.thresholds[sensor_type]
                if value < threshold['min'] or value > threshold['max']:
                    return True
            return False
        
        # Register UDF
        check_threshold = udf(check_threshold_udf, BooleanType())
        
        df_with_alerts = df.withColumn(
            "threshold_violation",
            check_threshold(col("sensor_type"), col("value"))
        )
        
        # Calculate severity
        df_with_alerts = df_with_alerts.withColumn(
            "severity",
            when(col("threshold_violation") & col("is_anomaly"), "critical")
            .when(col("threshold_violation"), "high")
            .when(col("is_anomaly"), "medium")
            .otherwise("low")
        )
        
        return df_with_alerts
    
    def write_to_postgres(self, df, table_name, mode="append"):
        """Εγγραφή δεδομένων στο PostgreSQL"""
        
        def write_batch(batch_df, batch_id):
            logger.info(f"Εγγραφή batch {batch_id} στο {table_name}")
            
            batch_df.write \
                .format("jdbc") \
                .option("url", self.postgres_url) \
                .option("dbtable", table_name) \
                .option("user", self.postgres_user) \
                .option("password", self.postgres_password) \
                .option("driver", "org.postgresql.Driver") \
                .mode(mode) \
                .save()
            
            logger.info(f"Batch {batch_id} εγγράφηκε επιτυχώς")
        
        return write_batch
    
    def process_sensor_readings(self, df):
        """Επεξεργασία sensor readings και εγγραφή στη βάση"""
        
        # Prepare data για sensor_readings table
        readings_df = df.select(
            col("sensor_id"),
            col("timestamp"),
            col("sensor_type"),
            col("value"),
            col("unit"),
            col("is_anomaly"),
            col("location_lat"),
            col("location_lon"),
            col("city"),
            current_timestamp().alias("created_at")
        )
        
        # Write stream to PostgreSQL
        query = readings_df \
            .writeStream \
            .foreachBatch(self.write_to_postgres(readings_df, "sensor_readings")) \
            .outputMode("append") \
            .start()
        
        logger.info("Sensor readings stream ξεκίνησε")
        return query
    
    def generate_aggregated_metrics(self, df):
        """Δημιουργία συγκεντρωτικών μετρικών"""
        
        # Hourly aggregations
        hourly_agg = df \
            .withWatermark("timestamp", "1 hour") \
            .groupBy(
                window(col("timestamp"), "1 hour"),
                col("sensor_id"),
                col("sensor_type")
            ) \
            .agg(
                avg("value").alias("avg_value"),
                min("value").alias("min_value"),
                max("value").alias("max_value"),
                stddev("value").alias("stddev_value"),
                count("*").alias("count_readings"),
                count(when(col("is_anomaly"), 1)).alias("anomaly_count")
            ) \
            .select(
                col("sensor_id"),
                col("sensor_type"),
                expr("'hourly'").alias("aggregation_type"),
                col("window.start").alias("time_bucket"),
                col("avg_value"),
                col("min_value"),
                col("max_value"),
                col("stddev_value"),
                col("count_readings"),
                col("anomaly_count"),
                current_timestamp().alias("created_at")
            )
        
        # Write aggregated metrics
        query = hourly_agg \
            .writeStream \
            .foreachBatch(self.write_to_postgres(hourly_agg, "aggregated_metrics")) \
            .outputMode("append") \
            .start()
        
        logger.info("Aggregated metrics stream ξεκίνησε")
        return query
    
    def generate_alerts(self, df):
        """Δημιουργία alerts για anomalies και threshold violations"""
        
        # Filter για alerts
        alerts_df = df.filter(
            (col("is_anomaly") == True) | (col("threshold_violation") == True)
        ).select(
            col("sensor_id"),
            expr("CASE WHEN threshold_violation THEN 'threshold' ELSE 'anomaly' END").alias("alert_type"),
            col("severity"),
            expr("CONCAT('Sensor ', sensor_type, ' value: ', value, ' ', unit)").alias("message"),
            col("value").alias("actual_value"),
            col("timestamp").alias("triggered_at"),
            current_timestamp().alias("created_at")
        )
        
        # Write alerts
        query = alerts_df \
            .writeStream \
            .foreachBatch(self.write_to_postgres(alerts_df, "alerts")) \
            .outputMode("append") \
            .start()
        
        logger.info("Alerts stream ξεκίνησε")
        return query
    
    def start_processing(self):
        """Εκκίνηση όλων των streaming processes"""
        
        try:
            # Initialize Spark
            self.initialize_spark()
            
            # Read Kafka stream
            raw_df = self.read_kafka_stream()
            
            # Apply transformations
            df_with_anomalies = self.detect_anomalies(raw_df)
            df_with_alerts = self.check_thresholds(df_with_anomalies)
            
            # Start processing queries
            readings_query = self.process_sensor_readings(df_with_alerts)
            metrics_query = self.generate_aggregated_metrics(df_with_alerts)
            alerts_query = self.generate_alerts(df_with_alerts)
            
            # Console output για debugging
            console_query = df_with_alerts \
                .select("sensor_id", "sensor_type", "value", "is_anomaly", "severity") \
                .writeStream \
                .outputMode("append") \
                .format("console") \
                .option("truncate", False) \
                .start()
            
            logger.info("Όλα τα streaming queries ξεκίνησαν επιτυχώς")
            
            # Αναμονή για termination
            self.spark.streams.awaitAnyTermination()
            
        except Exception as e:
            logger.error(f"Σφάλμα στο streaming processing: {e}")
            raise
        finally:
            if self.spark:
                self.spark.stop()
                logger.info("Spark session έκλεισε")


def main():
    """Main entry point"""
    
    processor = SmartCityStreamProcessor(
        kafka_servers="localhost:9092",
        postgres_url="jdbc:postgresql://localhost:5432/smartcity",
        postgres_user="smartcity_user",
        postgres_password="SmartCity2024!"
    )
    
    logger.info("Εκκίνηση Smart City Stream Processor...")
    processor.start_processing()


if __name__ == "__main__":
    main()
---
